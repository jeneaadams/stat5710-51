---
title: "AT models"
author:
- Jenea Adams
- Annan Timon
date: 'april 22'
output:
  html_document:
    code_folding: show
    highlight: haddock
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue  
---

# Packages 


```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height=4, fig.width=7, fig.align = 'center', warning = F)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(sf, readxl, tidyverse, ggplot2, ggpmisc, ggpubr, censusxy, tigris, ndi, dplyr,
               rmapshaper,tmap, areal,leaflet, RColorBrewer,raster, rasterVis, tidycensus, censusapi,
               reshape2, wesanderson, ggarrange, viridis, ggsci, mapview) 
```

```{r}
census_disp_final <- readRDS("data/final_data.RDS")
```


```{r}
health_columns <- c(1, grep("[A-Z]*_CrudePrev",colnames(census_disp_final)))
demog_columns <- colnames(census_disp_final)[grep("COUNT_.*|PERCENT_.*",colnames(census_disp_final))]
Ys <- colnames(census_disp_final)[health_columns]
features <- colnames(census_disp_final)[!(colnames(census_disp_final) %in% Ys) &
                                        !(colnames(census_disp_final) %in% demog_columns)]

Ys
features
```
#### ignore NN stuff for now

```{r}

test_model_diab <- census_disp_final %>%
  dplyr::select(DIABETES_CrudePrev,features)

test_model_chol <- census_disp_final %>%
  dplyr::select(HIGHCHOL_CrudePrev,features)

test_model_obes <- census_disp_final %>%
  dplyr::select(OBESITY_CrudePrev,features)

test_model_chd <- census_disp_final %>%
  dplyr::select(CHD_CrudePrev,features)


```

# test linear models
```{r}
#diabetes test, we should make a feature by disease matrix. If the features are significant show the magnitude of the coeifficient
diab <-lm(DIABETES_CrudePrev~., test_model_diab)
summary(diab)

chol <-lm(HIGHCHOL_CrudePrev~., test_model_chol)
summary(chol)

obes <-lm(OBESITY_CrudePrev~., test_model_obes)
summary(obes)

chd <-lm(CHD_CrudePrev~., test_model_chd)
summary(chd)


coef(summary(diab)) %>% 
  data.frame() %>%
  mutate_if()

diab
```


### Neural network

i. Let's specify an architecture with the following specifications
  a) One hidden layers with 20 neurons
  b) Relu activation function
  c) Softmax output
  d) Explain in a high level what is the model? How many unknown weights (parameters are there)


**Compile the Model**

```{r}
# specify a function to run our neural network
# inputs: data frame, percent of validation
run_nn <- function(data, split) {
  # Split data
  set.seed(1)  # for the purpose of reproducibility
  test.index <- sample(n, nrow(data)*split)
  
  data_val <- data[test.index, ]
  
  ## validation input/y
  data_xval <- as.matrix(data_val[, -1])  # make sure it it is a matrix
  data_yval <- as.matrix(data_val[, 1]) # make sure it it is a matrix
  
  
}
```

  
```{r}
data 
```

```{r}
# Split data
set.seed(1)  # for the purpose of reproducibility

data_val <- data[test.index, ] # 

## validation input/y
data_xval <- as.matrix(data_val[, -1])  # make sure it it is a matrix
data_yval <- as.matrix(data_val[, 1]) # make sure it it is a matrix
dim(data_xval)
dim(data_yval)
```

```{r}
## training input/y: need to be matrix/vector
# only keep rating and the texts
data.nottest <- data[-test.index,]

data_xtrain <- data.nottest[-valid.index, -1]
data_ytrain <- data.nottest[-valid.index, 1]   
data_xtrain <- as.matrix(data_xtrain)
data_ytrain <- as.matrix(data_ytrain) 

dim(data_xtrain)
dim(data_ytrain)
```

ii. Train your model and call it `fit.nn` 
  a) using the training data
  b) split 85% vs. 15% internally
  c) find the optimal epoch
  
```{r}
p <- dim(data_xtrain)[2] # number of input variables
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(p)) %>% 
  # 1 layer with 16 neurons. default activation is relu
  layer_dense(units = 8, activation = "relu") %>%  
  # layer 2 with 8 neurons
  layer_dense(units = 2, activation = "softmax") # output

print(model)
```



```{r}
##Compile the Model
model %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = c("accuracy")
)
```


```{r}
fit.nn <- model %>% fit(
  data_xtrain,
  as.numeric(data_ytrain),
  epochs = 20, 
  batch_size = 512,
  validation_split = .15
)

plot(fit.nn)
```

_From the graph below we see that by about 6 epochs our validation loss has bottomed out and we receive no further benefit from additional iterations_

```{r}
weights <- model %>% get_weights()
# str(weights) # show each layers of W's and b's
# hist(weights[[1]])   # W^(1)
# weights[[2]] # b's for layer 1
```

```{r}
round(model %>% predict(data_xtrain[1:5,]), 3)  # 3 decimals
```

```{r}
n5 <- 5

# first layer: z_1 = W_1 X + b_1; a_1 = ReLU(z_1)
z_1 <- data_xtrain[1:n5, ] %*% weights[[1]] 
# add beta (weights[[2]]) to every row 
z_1 <- z_1 + matrix(rep(weights[[2]], n5), nrow = n5, byrow = T)
a_1 <- matrix(pmax(0, z_1), nrow = n5)

# second layer: z_2 = W_2 a_1 + b_2; a_2 = ReLU(z_2)
z_2 <- a_1 %*% weights[[3]]
z_2 <- z_2 + matrix(rep(weights[[4]], n5), nrow = n5, byrow = T)
a_2 <- matrix(pmax(0,  z_2), nrow = n5)

# output layer: softmax(W_3 a_2 + b_3)
z_out <- a_2 %*% weights[[5]] 
z_out <- z_out + matrix(rep(weights[[6]], n5), nrow = n5, byrow = T)
prob.pred <- t(apply(z_out, 1, function(z) exp(z)/sum(exp(z))))

round(prob.pred, 3)
```

```{r}
fit.nn$metrics$loss[20]
```


```{r}
p <- dim(data_xtrain)[2] # number of input variables

#retain the nn:
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(p)) %>% 
  # 1 layer with 16 neurons. default activation is relu
  layer_dense(units = 8, activation = "relu") %>%  # layer 2 with 8 neurons
  layer_dense(units = 2, activation = "softmax") # output

model %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = c("accuracy")
)

 model %>% fit(data_xtrain, as.numeric(data_ytrain), epochs = 6, batch_size = 512)
```

```{r}
results <- model %>% evaluate(data_xval, as.numeric(data_yval)) ; 
results
```


```{r}
pred.prob <- model %>% predict(data_xval[1:5,])
pred.prob
```

  
iii. Report the testing errors using majority vote. 

_Testing error 0.1234_

```{r}
y.pred <- model %>% predict(data_xval) %>% k_argmax() %>% as.integer()

mean(data_yval[,1] != y.pred)
```
