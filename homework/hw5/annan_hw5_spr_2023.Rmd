---
title: "Modern Data Mining - HW 5"
author:
- Jenea Adams
- Annan Timon
date: 'Due: 11:59Pm,  4/16, 2023'
output:
  html_document:
    code_folding: show
    highlight: haddock
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height=4, fig.width=7, fig.align = 'center', warning = F)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(randomForest, tree, ISLR, rpart, rattle, pROC, partykit,
               ggplot2, glmnet, leaps, dplyr, keras, neuralnet, imager,
               ranger, jsonlite, tm, SnowballC, RColorBrewer, wordcloud,
               data.table, tensorflow, neuralnet,nplotly, latex2exp, lda,
               rpart, rattle, partykit,car) 
```




# Overview

For the purpose of predictions, a nonlinear model or a model free approach could be beneficial. 

Neural networks are a good way to do prediction because they are capable of learning complex patterns and relationships within data, without requiring explicit programming of rules. They are able to recognize non-linear relationships and make predictions based on those patterns, which can be very difficult to accomplish with traditional statistical models. Neural networks can also be used for a wide range of prediction tasks, including image and speech recognition, natural language processing, and time series forecasting.

Neural networks have been around for several decades, but they have become more popular and widely used in recent years due to several factors. One reason is the increase in available data, which allows for more robust and accurate training of neural networks. Additionally, advancements in hardware technology have made it easier and more cost-effective to train large neural networks. Another reason is the development of new neural network architectures, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), which have proven to be very effective for specific tasks. Finally, the rise of deep learning, which is a subfield of machine learning that utilizes deep neural networks with many layers, has further propelled the popularity of neural networks due to their ability to achieve state-of-the-art performance on a wide range of tasks.Neural Network is a natural extension of the linear models. 

On a model free side, a binary decision tree is the simplest, still interpretable and often provides insightful information between predictors and responses. To improve the predictive power we would like to aggregate many equations, especially uncorrelated ones. One clever way to have many free samples is to take bootstrap samples. For each bootstrap sample we  build a random tree by taking a randomly chosen number of variables to be split at each node. We then take average of all the random bootstrap trees to have our final prediction equation. This is RandomForest. 

Ensemble method can be applied broadly: simply take average or weighted average of many different equations. This may beat any single equation in your hand.


All the methods covered can handle both continuous responses as well as categorical response with multiple levels (not limited to binary response.)


## Objectives

- Understand a basic NN 
  + Be able to write an architecture (a model)
  + Understand the roles of 
    - Hidden layers
    - Neurons
    - Relu activation function
    
  + Be able to run keras to train and to use a NN. 

- Understand trees
    + single tree/displaying/pruning a tree
    + RandomForest
    + Ensemble idea

- R functions/Packages
    + `kears`
    + `tree`, `RandomForest`, `ranger`
    
- Json data format

- text mining
    + bag of words
  

Data needed:

+ `yelp_review_20k.json`
+ `IQ.Full.csv`


# Problem 0: Lectures

Please study all the lectures. Understand the main elements in each lecture and be able to run and compile the lectures

+ textmining
+ deep learning
+ trees
+ boosting



# Problem 1: Yelp challenge 2019

**Note:** This problem is rather involved. It covers essentially all the main materials we have done so far in this semester. It could be thought as a guideline for your final project if you want when appropriate. 

Yelp has made their data available to public and launched Yelp challenge. [More information](https://www.yelp.com/dataset/). It is unlikely we will win the $5,000 prize posted but we get to use their data for free. We have done a detailed analysis in our lecture. This exercise is designed for you to get hands on the whole process. 

For this case study, we downloaded the [data](https://www.yelp.com/dataset/download) and took a 20k subset from **review.json**. *json* is another format of a data. It is flexible and commonly-used for websites. Each item/subject/sample is contained in a brace *{}*. Data is stored as **key-value** pairs inside the brace. *Key* is the counterpart of column name in *csv* and *value* is the content/data. Both *key* and *value* are quoted. Each pair is separated by a comma. The following is an example of one item/subject/sample.

```{json}
{
  "key1": "value1",
  "key2": "value2"
}
```


**Data needed:** yelp_review_20k.json available in Canvas.

**yelp_review_20k.json** contains full review text data including the user_id that wrote the review and the business_id the review is written for. Here's an example of one review.

```{json}
{
    // string, 22 character unique review id
    "review_id": "zdSx_SD6obEhz9VrW9uAWA",

    // string, 22 character unique user id, maps to the user in user.json
    "user_id": "Ha3iJu77CxlrFm-vQRs_8g",

    // string, 22 character business id, maps to business in business.json
    "business_id": "tnhfDv5Il8EaGSXZGiuQGg",

    // integer, star rating
    "stars": 4,

    // string, date formatted YYYY-MM-DD
    "date": "2016-03-09",

    // string, the review itself
    "text": "Great place to hang out after work: the prices are decent, and the ambience is fun. It's a bit loud, but very lively. The staff is friendly, and the food is good. They have a good selection of drinks.",

    // integer, number of useful votes received
    "useful": 0,

    // integer, number of funny votes received
    "funny": 0,

    // integer, number of cool votes received
    "cool": 0
}
```

## Goal of the study

The goals are 

1) Try to identify important words associated with positive ratings and negative ratings. Collectively we have a sentiment analysis.  

2) To predict ratings using different methods. 

## JSON data and preprocessing data

i. Load *json* data

The *json* data provided is formatted as newline delimited JSON (ndjson). It is relatively new and useful for streaming.
```{json}
{
  "key1": "value1",
  "key2": "value2"
}
{
  "key1": "value1",
  "key2": "value2"
}
```

The traditional JSON format is as follows.
```{json}
[{
  "key1": "value1",
  "key2": "value2"
},
{
  "key1": "value1",
  "key2": "value2"
}]
```


We use `stream_in()` in the `jsonlite` package to load the JSON data (of ndjson format) as `data.frame`. (For the traditional JSON file, use `fromJSON()` function.)

```{r, echo=FALSE}
pacman::p_load(jsonlite)
yelp_data <- jsonlite::stream_in(file("data/yelp_review_20k.json"), verbose = F)
str(yelp_data)  
# different JSON format
# tmp_json <- toJSON(yelp_data[1:10,])
# fromJSON(tmp_json)
```



```{r}
yelp_data$rating <- c(0)
yelp_data$rating[yelp_data$stars >= 4] <- 1
yelp_data$rating <- as.factor(yelp_data$rating)
summary(yelp_data)
```


```{r}
prop.table(table(yelp_data$stars))
```


```{r}
# Take a small set to work through
# MAKE SURE: you will rerun the analyses later by
# setting back a larger dataset.

# yelp_sub <- yelp_data[1:1000, ]
# names(yelp_sub)
# str(yelp_sub)   
# 
# #someone wrote more than one revews
# length(unique(yelp_sub$user_id))
# length(unique(yelp_sub$review_id))
# n <- nrow(yelp_sub)
# levels(as.factor(yelp_sub$stars))
# 
# 
# yelp_sub$stars <- as.factor(yelp_sub$stars)
# summary(yelp_sub)
```


**Write a brief summary about the data:**

_The data is a small sample (n=19,999) of all the reviews for different restaurants. It contains a review, information about the user who wrote the review, which restaurant the review is about, a 5 star based rating system, and different scores describing how useful, funny, or cool the review was.See http://www.yelp.com/dataset_challenge for more information. _

a) Which time period were the reviews collected in this data?

_Reviews were collected from 10/19/2004 to 10/04/2018_

```{r}
range(as.Date(yelp_data$date))
```


b) Are ratings (with 5 levels) related to month of the year or days of the week? Only address this through EDA please. 

_Ratings do not appear to be related to the month of the year or days of the week. Reviews are not biased towards the weekend or weekday, from the plots and analysis below we see an even distribution of reviews across days and months._

```{r}
weekdays <- weekdays(as.Date(yelp_data$date)) # get weekdays for each review  
months <- months(as.Date(yelp_data$date))   # get months 
```


```{r results='hold', fig.height=5, fig.width=10}
par(mfrow=c(1,2))
pie(table(weekdays), main="Prop of reviews") # Pretty much evenly distributed
pie(table(months))  
```


```{r results='hold'}
prop.table(table(yelp_data$stars, months), 2)  # prop of the columns
prop.table(table(yelp_data$stars, months), 1)  # prop of the rows

## maybe visualize this as a ggbarplot
```

```{r results='hold'}
prop.table(table(yelp_data$stars, weekdays), 2)  # prop of the columns
prop.table(table(yelp_data$stars, weekdays), 1)  # prop of the rows

## maybe visualize this as a ggbarplot
```

ii. Document term matrix (dtm) (bag of words)
 
 Extract document term matrix for texts to keep words appearing at least .5% of the time among all 20000 documents. Go through the similar process of cleansing as we did in the lecture. 
 
```{r, results = FALSE}
mycorpus1 <- VCorpus(VectorSource(yelp_data$text))
mycorpus1
typeof(mycorpus1)   ## It is a list
# inspect the first corpus
inspect(mycorpus1[[1]])
# or use `as.character` to extract the text
as.character(mycorpus1[[1]])
```

```{r, results = FALSE}
## inspect corpus 4,5
lapply(mycorpus1[4:5], as.character)
```


```{r n-gram tokenizer}
# The 'n' for n-grams
# n=2 is bi-grams
n <- 2

# Our custom tokenizer
# Uses the ngrams function from the NLP package
# Right now this is for bigrams, but you can change it by changing the value of
# the variable n (includes N-grams for any N <= n)
ngram_tokenizer <- function(x, n) {
  unlist(lapply(ngrams(words(x), 1:n), paste, collapse = "_"), use.names = FALSE)
}
```


We next prepare a clean copus
```{r}
# Converts all words to lowercase
mycorpus1  <- VCorpus(VectorSource(yelp_data$text))

mycorpus_clean <- tm_map(mycorpus1, content_transformer(tolower))

# Removes common English stopwords (e.g. "with", "i")
mycorpus_clean <- tm_map(mycorpus_clean, removeWords, stopwords("english"))

# Removes any punctuation
# NOTE: This step may not be appropriate if you want to account for differences
#       on semantics depending on which sentence a word belongs to if you end up
#       using n-grams or k-skip-n-grams.
#       Instead, periods (or semicolons, etc.) can be replaced with a unique
#       token (e.g. "[PERIOD]") that retains this semantic meaning.
mycorpus_clean <- tm_map(mycorpus_clean, removePunctuation)

# Removes numbers
mycorpus_clean <- tm_map(mycorpus_clean, removeNumbers)

# Stem words
mycorpus_clean <- tm_map(mycorpus_clean, stemDocument, lazy = TRUE)  
```



Apply the bigram tokenizer to the first review, we see the library or word of bag is enlarged.
```{r, , results = FALSE}
inspect(mycorpus_clean[[1]])  # see review 1 again
ngram_tokenizer(mycorpus_clean[[1]], 2) # output bigram of review 1
```


```{r n-gram DTM, , results = FALSE}
# use ngram_tokenizer()
control_list_ngram <- list(tokenize = function(x) ngram_tokenizer(x, 2))

dtm_ngram_long <- DocumentTermMatrix(mycorpus_clean, control_list_ngram)
# kick out rare words 
dtm_ngram <- removeSparseTerms(dtm_ngram_long, 1-.005)  
inspect(dtm_ngram)
```


a) Briefly explain what does this matrix record? What is the cell number at row 100 and column 405? What does it represent?

_For each review (represented by the rows of the matrix), this matrix records the frequency or count of each word (denoted by the columns)._

```{r, results = FALSE}
inspect(dtm_ngram[100,405])
```


b) What is the sparsity of the dtm obtained here? What does that mean?

_Sparsity refers to the values of the matrix that are 0. Most words will not appear in every single review so there will be many 0 values resulting in a matrix that is sparse._

c) Set the stars as a two category response variable called rating to be “1” = 5,4 and “0”= 1,2,3. Combine the variable rating with the dtm as a data frame called data2. 

```{r, results = FALSE}
names(yelp_data)

# Combine the original data with the text matrix
data1.temp <- data.frame(yelp_data,as.matrix(dtm_ngram) )   
dim(data1.temp)
head(data1.temp)
```

```{r, results = FALSE}
names(data1.temp)[1:30]
str(data1.temp)
# data2 consists of date, rating and all the top 1% words
```

```{r, results = FALSE}
data2 <- data1.temp[, c(10:ncol(data1.temp))]
names(data2)[1:20]
dim(data2)

#### We have previously run the entire 20,000 rows and output the DTM out. 
### if not, run and write as csv
if(!file.exists("data/YELP_tm_freq.csv")) {
  fwrite(data2, "data/YELP_tm_freq.csv", row.names = FALSE)
}
```


## Analyses

Get a training data with 13000 reviews and the 5000 reserved as the testing data. Keep the rest (2000) as our validation data set. 


As one standard machine learning process, we first split data into two sets one training data and the other testing data. We use training data to build models, choose models etc and make final recommendations. We then report the performance using the testing data.

Reserve 5000 randomly chosen rows as our test data (`data2.test`), 13000 as the training set, and the remaining 2000 as as our validation set (`data2.train`)

```{r, results = FALSE, echo=FALSE}
table(data2$rating)
object.size(data2)
```

```{r}
 # for the purpose of reproucibility
set.seed(1) 
n <- nrow(data2)
test.index <- sample(n, 5000)

# only keep rating and the texts
data2.test <- data2[test.index, ]
data2.nottest <- data2[-test.index,]

n2 <- nrow(data2.nottest)
valid.index <- sample(n2, 2000)
data2.valid <- data2.nottest[valid.index,]
data2.train <- data2.nottest[-valid.index,]

dim(data2.test)
dim(data2.train)
dim(data2.valid)

```

### LASSO

i. Use the training data to get Lasso fit. Choose lambda.1se. Label the the fit as fit.lasso. Comment on what tuning parameters are chosen at the end and why?

We first explore a logistic regression model using LASSO. The following R-chunk runs a LASSO model with $\alpha=.99$. The reason we take an elastic net is to enjoy the nice properties from both `LASSO` (impose sparsity) and `Ridge` (computationally stable). 

LASSO takes sparse design matrix as an input. So make sure to extract the sparse matrix first as the input in cv.glm(). It takes about 1 minute to run cv.glm() with sparse matrix or 11 minutes using the regular design matrix. 


```{r}
### or try `sparse.model.matrix()` which is much faster
y <- data2.train$rating
X1 <- sparse.model.matrix(rating~., data=data2.train)[, -1]

set.seed(1)

# not running this because it takes too much time in order to knit
fit.lasso <- cv.glmnet(X1, y, alpha=.99, family="binomial")

plot(fit.lasso)
```

```{r}
coef.1se <- coef(fit.lasso, s="lambda.1se")  
coef.1se <- coef.1se[which(coef.1se !=0),] 
lasso.words <- rownames(as.matrix(coef.1se))[-1]
summary(lasso.words)

head(lasso.words)

### cv.glmnt with the non-sparse design matrix takes much longer
# X <- as.matrix(data2.train[, -1]) # we can use as.matrix directly her
#### Be careful to run the following LASSO.
#set.seed(2)
#result.lasso <- cv.glmnet(X, y, alpha=.99, family="binomial")  
# 10 minutes in my MAC
#plot(result.lasso)

# this this may take you long time to run, we save result.lasso
saveRDS(fit.lasso, file="data/TextMiningHW_lasso.RDS")
```

ii. Feed the output from Lasso above, get a logistic regression and call this fit.glm

```{r relax lasso}
sel_cols <- c("rating", lasso.words)
# use all_of() to specify we would like to select variables in sel_cols
data_sub <- data2.train %>% dplyr::select(all_of(sel_cols))

fit.glm <- glm(rating~., family=binomial, data_sub)

## glm() returns a big object with unnecessary information
saveRDS(fit.glm, file = "data/TextMiningHW_glm.RDS")

## trim the glm() fat from 
## https://win-vector.com/2014/05/30/trimming-the-fat-from-glm-models-in-r/
stripGlmLR = function(cm) {
  cm$y = c()
  cm$model = c()
  
  cm$residuals = c()
  cm$fitted.values = c()
  cm$effects = c()
  cm$qr$qr = c()  
  cm$linear.predictors = c()
  cm$weights = c()
  cm$prior.weights = c()
  cm$data = c()

  
  cm$family$variance = c()
  cm$family$dev.resids = c()
  cm$family$aic = c()
  cm$family$validmu = c()
  cm$family$simulate = c()
  attr(cm$terms,".Environment") = c()
  attr(cm$formula,".Environment") = c()
  
  cm
}

fit.glm.small <- stripGlmLR(fit.glm)

saveRDS(fit.glm.small, 
     file = "data/TextMiningHW_glm_small.RDS")
```
	
a) Pull out all the positive coefficients and the corresponding words. Rank the coefficients in a decreasing order. Report the leading 2 words and the coefficients. Describe briefly the interpretation for those two coefficients. 

_The 2 leading words are thorough and worth_wait. The words here are all largely positive and speak to useful qualities of a restaurant that might lead to great reviews._

```{r, warning=FALSE}
fit.glm.small <- readRDS("data/TextMiningHW_glm_small.RDS")
fit.glm.small.coef <- coef(fit.glm.small)
#fit.glm.small.coef[200:250]
#hist(fit.glm.small.coef)

# pick up the positive coef's which are positively related to the prob of being a good review
good.glm <- fit.glm.small.coef[which(fit.glm.small.coef > 0)]
good.glm <- good.glm[-1]  # took intercept out
#names(good.glm)[1:20]  # which words are positively associated with good ratings

good.fre <- log(sort(good.glm, decreasing = TRUE)) # sort the coef's
round(good.fre, 4)[1:2] # leading 20 positive words, amazing!
#length(good.fre)

# hist(as.matrix(good.fre), breaks=30, col="red") 
good.word <- names(good.fre)  # good words with a decreasing order in the coeff's
```



b) Make a word cloud with the top 100 positive words according to their coefficients. Interpret the cloud briefly.

_To fit all the words we decided to do the log of all the weights. The chunk below shows in detail the weight for positive words. The words here are all largely positive and speak to useful qualities of a restaurant that might lead to great reviews._

```{r results=TRUE, warning=FALSE, message=FALSE, fig.height=10, fig.width=12}
cor.special <- brewer.pal(8,"Dark2")
wordcloud(good.word[1:100], good.fre[1:100],  # make a word cloud
          colors=cor.special, ordered.colors=F)
```


c) Repeat a) and b) above for the bag of negative words.

_The 2 leading words are disgust and unproffession. The words here are all largely negative and speak to qualities of a restaurant that might lead to bad reviews._

```{r, warning=FALSE}
# pick up the positive coef's which are negatively related to the prob of being a good review
bad.glm <- fit.glm.small.coef[which(fit.glm.small.coef < 0)]
bad.glm <- bad.glm[-1]  # took intercept out

bad.fre <- log(sort(-bad.glm, decreasing = TRUE) )
round(bad.fre, 4)[1:2]

# hist(as.matrix(good.fre), breaks=30, col="red") 
bad.word <- names(bad.fre)  # good words with a decreasing order in the coeff's
length(bad.fre)
```

_To fit all the words we decided to do the log of all the weights. The chunk below shows in detail the weight for negative words. The words here are all largely negative and speak to qualities of a restaurant that might lead to bad reviews._

```{r results=TRUE, warning=FALSE, message=FALSE, fig.height=10, fig.width=12}
cor.special <- brewer.pal(8,"Dark2")
wordcloud(bad.word[1:100], bad.fre[1:100],  # make a word cloud
          colors=cor.special, ordered.colors=F)
```

d) Summarize the findings. 

_We apply LASSO to classify good/bad review based on the text. The core technique for text mining is a simple bag of words, i.e. a word frequency matrix. The problem becomes a high-dimensional problem. Using LASSO, we reduce dimension and train a model with high predictive power. Based on the model, we find out the positive/negative words and build a word cloud._

iii. What are the major differences among the two methods used so far: Lasso and glm

_Comparing the two models we do not see much of the difference. We could use either final models for the purpose of the prediction._

iv.  Using majority votes find the testing errors
	a) From `fit.lasso`
  b) From `fit.glm`
	c) Which one is smaller?

_Lasso test error is smaller_
	
```{r results= 'hold'}
predict.glm <- predict(fit.glm, data2.test, type = "response")
class.glm <- ifelse(predict.glm > .5, "1", "0")
# length(class.glm)

testerror.glm <- mean(data2.test$rating != class.glm)
testerror.glm   # mis classification error is 

predict.lasso.p <- predict(fit.lasso, as.matrix(data2.test[, -1]), type = "response", s="lambda.1se")

# output lasso estimates of prob's
predict.lasso <- predict(fit.lasso, as.matrix(data2.test[, -1]), type = "class", s="lambda.1se")

# LASSO testing errors
testerror.lasso <-mean(data2.test$rating != predict.lasso)   # 
testerror.lasso

# ROC curve for LASSO estimates

```

```{r, message=FALSE}
fit.lasso.roc <- roc(data2.test$rating, predict.lasso.p)
fit.glm.roc <- roc(data2.test$rating, predict.glm)


plot(1-fit.lasso.roc$specificities,
     fit.lasso.roc$sensitivities, col="black", lwd=3, type="l",
     xlab="Specificity",
     ylab="Sensitivity")
lines(1-fit.glm.roc$specificities,
     fit.glm.roc$sensitivities, col="blue", lwd=3)
legend("bottomright",
       c(paste0("fit.lasso AUC=", round(fit.lasso.roc$auc,2)),
        paste0("fit.glm AUC=", round(fit.glm.roc$auc,2))),
       col=c("black","blue"),
       lty=1)

```

### Neural network

i. Let's specify an architecture with the following specifications
  a) One hidden layers with 20 neurons
  b) Relu activation function
  c) Softmax output
  d) Explain in a high level what is the model? How many unknown weights (parameters are there)
  
As we can see in the table below our model has a total of **24954** parameters:    

  + The input to the model are yelp reviews that are each coded as 1072 length sequences of frequencies. 
  + Our model's first layer is 16 nodes that are fully connected    
  + At each node a different set of weights $W's$ will be applied to each value of the 1549 word sequence (i.e. each node will have 1549 weights and the network will have (1549 +1) * 16 = 24800 weights total) to compute the weighted sum to which a bias will be added and then the activation function will be applied.  
  + These values will then flow to our second layer with (16+1)*8=136 where weights will be applied to each value (16 weights total) and the weighted sum computed, the bias value will then be added and the activation function will then be applied 
  + The final layer which is output will have (8+1)*2=18 parameters
  + Combined our model with two layers and one final output, there are a total of 24800+136+18=24954 parameters across the model or architecture.   


**Compile the Model**
  
```{r}
data3 <- data2
dim(data3)
```

```{r}
# Split data
set.seed(1)  # for the purpose of reproducibility

data3_val <- data3[test.index, ] # 

## validation input/y
data3_xval <- as.matrix(data3_val[, -1])  # make sure it it is a matrix
data3_yval <- as.matrix(data3_val[, 1]) # make sure it it is a matrix
dim(data3_xval)
dim(data3_yval)
```

```{r}
## training input/y: need to be matrix/vector
# only keep rating and the texts
data3.nottest <- data3[-test.index,]

data3_xtrain <- data3.nottest[-valid.index, -1]
data3_ytrain <- data3.nottest[-valid.index, 1]   
data3_xtrain <- as.matrix(data3_xtrain)
data3_ytrain <- as.matrix(data3_ytrain) 

dim(data3_xtrain)
dim(data3_ytrain)
```

ii. Train your model and call it `fit.nn` 
  a) using the training data
  b) split 85% vs. 15% internally
  c) find the optimal epoch
  
```{r}
p <- dim(data3_xtrain)[2] # number of input variables
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(p)) %>% 
  # 1 layer with 16 neurons. default activation is relu
  layer_dense(units = 8, activation = "relu") %>%  
  # layer 2 with 8 neurons
  layer_dense(units = 2, activation = "softmax") # output

print(model)
```



```{r}
##Compile the Model
model %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = c("accuracy")
)
```


```{r}
fit.nn <- model %>% fit(
  data3_xtrain,
  as.numeric(data3_ytrain),
  epochs = 20, 
  batch_size = 512,
  validation_split = .15
)

plot(fit.nn)
```

_From the graph below we see that by about 6 epochs our validation loss has bottomed out and we receive no further benefit from additional iterations_

```{r}
weights <- model %>% get_weights()
# str(weights) # show each layers of W's and b's
# hist(weights[[1]])   # W^(1)
# weights[[2]] # b's for layer 1
```

```{r}
round(model %>% predict(data3_xtrain[1:5,]), 3)  # 3 decimals
```

```{r}
n5 <- 5

# first layer: z_1 = W_1 X + b_1; a_1 = ReLU(z_1)
z_1 <- data3_xtrain[1:n5, ] %*% weights[[1]] 
# add beta (weights[[2]]) to every row 
z_1 <- z_1 + matrix(rep(weights[[2]], n5), nrow = n5, byrow = T)
a_1 <- matrix(pmax(0, z_1), nrow = n5)

# second layer: z_2 = W_2 a_1 + b_2; a_2 = ReLU(z_2)
z_2 <- a_1 %*% weights[[3]]
z_2 <- z_2 + matrix(rep(weights[[4]], n5), nrow = n5, byrow = T)
a_2 <- matrix(pmax(0,  z_2), nrow = n5)

# output layer: softmax(W_3 a_2 + b_3)
z_out <- a_2 %*% weights[[5]] 
z_out <- z_out + matrix(rep(weights[[6]], n5), nrow = n5, byrow = T)
prob.pred <- t(apply(z_out, 1, function(z) exp(z)/sum(exp(z))))

round(prob.pred, 3)
```

```{r}
fit.nn$metrics$loss[20]
```


```{r}
p <- dim(data3_xtrain)[2] # number of input variables

#retain the nn:
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(p)) %>% 
  # 1 layer with 16 neurons. default activation is relu
  layer_dense(units = 8, activation = "relu") %>%  # layer 2 with 8 neurons
  layer_dense(units = 2, activation = "softmax") # output

model %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = c("accuracy")
)

 model %>% fit(data3_xtrain, as.numeric(data3_ytrain), epochs = 6, batch_size = 512)
```

```{r}
results <- model %>% evaluate(data3_xval, as.numeric(data3_yval)) ; 
results
```


```{r}
pred.prob <- model %>% predict(data3_xval[1:5,])
pred.prob
```

  
iii. Report the testing errors using majority vote. 

_Testing error 0.1234_

```{r}
y.pred <- model %>% predict(data3_xval) %>% k_argmax() %>% as.integer()

mean(data3_yval[,1] != y.pred)
```


### Random Forest  

i. Briefly summarize the method of Random Forest

ii. Now train the data using the training data set by RF and call it `fit.rf`. 
  a) Explain how you tune the tuning parameters (`mtry` and `ntree`). 
  b) Get the testing error of majority vote. 
  
_Testing Error is 0.1662_

```{r }
# Reserve 1000 as test data
set.seed(1)
data4 <- data2
data4.test <- data4[test.index, ]
data4.nottest <- data4[-test.index,]


data4.train <- data4.nottest[-valid.index, ]

dim(data4.train)
dim(data4.test)
```


looking at oob mse for 100 trees
```{r }
nt <- seq(1, 100, 10)

oob_mse <- vector("numeric", length(nt))

for(i in 1:length(nt)){
  rf <- ranger::ranger(rating~., data4.train, mtry = 5, num.trees = nt[i], importance="impurity")
  oob_mse[i] <- rf$prediction.error
}


plot(x = nt, y = oob_mse, col = "red", type = "l")
```

looking at oob mse for 500 trees
```{r }
nt <- seq(1, 500, 10)

oob_mse <- vector("numeric", length(nt))

for(i in 1:length(nt)){
  rf <- ranger::ranger(rating~., data4.train, mtry = 5, num.trees = nt[i], importance="impurity")
  oob_mse[i] <- rf$prediction.error
}


plot(x = nt, y = oob_mse, col = "red", type = "l")
```

looking at oob mse for 200 trees
```{r }
nt <- seq(1, 200, 10)

oob_mse <- vector("numeric", length(nt))

for(i in 1:length(nt)){
  rf <- ranger::ranger(rating~., data4.train, mtry = 5, num.trees = nt[i], importance="impurity")
  oob_mse[i] <- rf$prediction.error
}


plot(x = nt, y = oob_mse, col = "red", type = "l")
```

looking at oob mse for 250 trees
```{r }
nt <- seq(1, 250, 10)

oob_mse <- vector("numeric", length(nt))

for(i in 1:length(nt)){
  rf <- ranger::ranger(rating~., data4.train, mtry = 5, num.trees = nt[i], importance="impurity")
  oob_mse[i] <- rf$prediction.error
}


plot(x = nt, y = oob_mse, col = "red", type = "l")
```

looking at oob mse for 200 and varying mtry
```{r}
rf.error.p <- 1:50  # set up a vector of length 50
for (p in 1:50) 
{
  rf <- ranger::ranger(rating~., data4.train, mtry = p, num.trees = 200, importance="impurity")
  
  rf.error.p[p] <-  rf$prediction.error  # collecting oob mse based on 250 trees
}
rf.error.p   

plot(1:50, rf.error.p, pch=16,
     main = "Testing errors of mtry with 250 trees",
     xlab="mtry",
     ylab="OOB mse of mtry")
lines(1:50, rf.error.p)
```

```{r}
fit.rf <- ranger::ranger(rating~., data4.train, mtry = 6, num.trees = 200, importance="impurity")

imp <- importance(fit.rf)
imp[order(imp, decreasing = T)][1:20]


predict.rf <- predict(fit.rf, data=data4.test, type="response") 
mean(data2.test$rating != predict.rf$predictions)
```

###  PCA first

i. Perform PCA (better to do sparse PCA) for the input matrix first. Decide how many PC's you may want to take and why.

_Selecting 3 PC's based on scree plot showing that most of the variance is explained by the first 3 PCs_

```{r}
# Get train/test data 

set.seed(1)
set.seed(1)
data5 <- data2

data5.test <- data5[test.index, ]
data5.nottest <- data5[-test.index, ]

data5.train <- data5.nottest[-valid.index, ]
dim(data5.test)
dim(data5.train)
```

```{r}
# Get pc's for training data

pc.train <- prcomp(data5.train[, -c(1)], scale=TRUE)  # Take the rating out

pc.train$center[1:10] # means for each word
hist(pc.train$center, breaks=50,
     col="blue",
     main="mean frequency of the words")  

```

```{r}
# How do PC's capture the variabilities of the entire dtm?
pc.train.imp <- t((summary(pc.train))$importance)   # this is a matrix
pc.train.imp <- as.data.frame(pc.train.imp) 
names(pc.train.imp) <- c("Sdev", "PVE", "CPVE")
attach(pc.train.imp)
#hist(Sdev)
par(mfrow=c(1,2))
plot(PVE, xlim=c(1, 50))
plot(CPVE, main="Scree plot of CPVE")  # It is completely increasing no breaks.
detach(pc.train.imp)
```

```{r}
# Extract PC scores
# choosing only the first 3 PCs based on the screeplot
pc.train.scores <- pc.train$x[,1:3]
dim(pc.train.scores) 
```

```{r}
pc.test.scores <- predict(pc.train, data5.test[, -(1)])[,1:3]  # get pc scores for testing data
```


ii. Pick up one of your favorate method above and build the predictive model with PC's. Say you use RandomForest.

LASSO
```{r eval=F}
y <- data5.train$rating
X <- as.matrix(pc.train.scores)    # pc scores for training data
fit.lasso.pc <- cv.glmnet(X, y, alpha=0, family="binomial")  # 5 minutes in my MAC
plot(fit.lasso.pc)
```


RF
```{r eval=F}
data_sub_pc <- data.frame("rating" = factor(y), X)
fit.rf.pc <- ranger::ranger(rating~., data_sub_pc,importance="impurity")
imp.pc <- importance(fit.rf.pc)
imp.pc[order(imp.pc, decreasing = T)]
```



iii. What is the testing error? Is this testing error better than that obtained using the original x's? 

_The testing error for both LASSO and RF was around .24, they both do worse than the original x's_

```{r}
# Testing errors with pc.test.scores

predict.lasso.pc <- predict(fit.lasso.pc, pc.test.scores, type = "response")
class.lasso.pc <- rep("0", nrow(data5.test))
class.lasso.pc[predict.lasso.pc > .5] ="1"

#rf
predict.rf.pc <- predict(fit.rf.pc, data=pc.test.scores, type="response") 
```

```{r}
testerror.lasso.pc <- mean(data5.test$rating != class.lasso.pc)
testerror.lasso.pc

# rf predict error
mean(data5.test$rating != predict.rf$predictions)

```



### Ensemble model

i. Take average of some of the  models built above (also try all of them) and this gives us the fit.em. Report it's testing error. (Do you have more models to be bagged, try it.)


## Final model

Which classifier(s) seem to produce the least testing error? Are you surprised? Report the final model and accompany the validation error. Once again this is THE only time you use the validation data set.  For the purpose of prediction, comment on how would you predict a rating if you are given a review (not a tm output) using our final model? 




# Problem 2: IQ and successes

## Background: Measurement of Intelligence 

Case Study:  how intelligence relates to one's future successes?

**Data needed: `IQ.Full.csv`**

ASVAB (Armed Services Vocational Aptitude Battery) tests have been used as a screening test for those who want to join the army or other jobs. 

Our data set IQ.csv is a subset of individuals from the 1979 National Longitudinal Study of 
Youth (NLSY79) survey who were re-interviewed in 2006. Information about family, personal demographic such as gender, race and education level, plus a set of ASVAB (Armed Services Vocational Aptitude Battery) test scores are available. It is STILL used as a screening test for those who want to join the army! ASVAB scores were 1981 and income was 2005. 

**Our goals:** 

+ Is IQ related to one's successes measured by Income?
+ Is there evidence to show that Females are under-paid?
+ What are the best possible prediction models to predict future income? 


**The ASVAB has the following components:**

+ Science, Arith (Arithmetic reasoning), Word (Word knowledge), Parag (Paragraph comprehension), Numer (Numerical operation), Coding (Coding speed), Auto (Automative and Shop information), Math (Math knowledge), Mechanic (Mechanic Comprehension) and Elec (Electronic information).
+ AFQT (Armed Forces Qualifying Test) is a combination of Word, Parag, Math and Arith.
+ Note: Service Branch requirement: Army 31, Navy 35, Marines 31, Air Force 36, and Coast Guard 45,(out of 100 which is the max!) 

**The detailed variable definitions:**

Personal Demographic Variables: 

 * Race: 1 = Hispanic, 2 = Black, 3 = Not Hispanic or Black
 * Gender: a factor with levels "female" and "male"
 * Educ: years of education completed by 2006
 
Household Environment: 
 
* Imagazine: a variable taking on the value 1 if anyone in the respondent’s household regularly read
	magazines in 1979, otherwise 0
* Inewspaper: a variable taking on the value 1 if anyone in the respondent’s household regularly read
	newspapers in 1979, otherwise 0
* Ilibrary: a variable taking on the value 1 if anyone in the respondent’s household had a library card
	in 1979, otherwise 0
* MotherEd: mother’s years of education
* FatherEd: father’s years of education

Variables Related to ASVAB test Scores in 1981 (Proxy of IQ's)

* AFQT: percentile score on the AFQT intelligence test in 1981 
* Coding: score on the Coding Speed test in 1981
* Auto: score on the Automotive and Shop test in 1981
* Mechanic: score on the Mechanic test in 1981
* Elec: score on the Electronics Information test in 1981

* Science: score on the General Science test in 1981
* Math: score on the Math test in 1981
* Arith: score on the Arithmetic Reasoning test in 1981
* Word: score on the Word Knowledge Test in 1981
* Parag: score on the Paragraph Comprehension test in 1981
* Numer: score on the Numerical Operations test in 1981

Variable Related to Life Success in 2006

* Income2005: total annual income from wages and salary in 2005. We will use a natural log transformation over the income.


**Note: All the Esteem scores shouldn't be used as predictors to predict income**

## 1. EDA: Some cleaning work is needed to organize the data. 

+ The first variable is the label for each person. Take that out.
+ Set categorical variables as factors. 
+ Make log transformation for Income and take the original Income out
+ Take the last person out of the dataset and label it as **Michelle**. 
+ When needed, split data to three portions: training, testing and validation (70%/20%/10%)
  - training data: get a fit
  - testing data: find the best tuning parameters/best models
  - validation data: only used in your final model to report the accuracy. 

```{r}
iq <- fread("data/IQ.Full.csv")

# check to see if there are any missing values or wrong entries
str(iq)
summary(iq)

# remove subject, make categorical variables into factors, change income to log
iq %<>%
  mutate(across(c(Imagazine, Inewspaper, Ilibrary, Gender, Race,
                  Esteem1, Esteem2, Esteem3, Esteem4, Esteem5,
                  Esteem6, Esteem7, Esteem8, Esteem9, Esteem10),
                factor)) %>%
  mutate(Income = log(Income2005)) %>%
  dplyr::select(-Subject, -Income2005)

# extract Michelle
Michelle <- iq[nrow(iq),]

# check size of df before and after removing
dim(iq)
iq %<>% 
  filter(!row_number() %in% nrow(iq))
dim(iq)
summary(iq)
```

```{r}
 # for the purpose of reproucibility
set.seed(1) 
n <- nrow(iq)
test.index.iq <- sample(n, n*.2)

# only keep rating and the texts
iq.test <- iq[test.index.iq, ]
iq.nottest <- iq[-test.index,]

n2 <- nrow(iq.nottest)
valid.index.iq <- sample(n2, n*.1)
iq.valid <- iq.nottest[valid.index.iq,]
iq.train <- iq.nottest[-valid.index.iq,]

dim(iq.test)
dim(iq.train)
dim(iq.valid)
```
## 2. Factors affect Income

We only use linear models to answer the questions below.

i. To summarize ASVAB test scores, create PC1 and PC2 of 10 scores of ASVAB tests and label them as
ASVAB_PC1 and ASVAB_PC2. Give a quick interpretation of each ASVAB_PC1 and ASVAB_PC2 in terms of the original 10 tests. 

_together pcs 1 and 2 capture about 70% of the variance observed_

```{r  message=F, echo =F}
asvab <- iq %>%
  dplyr::select(AFQT, Coding, Auto, Mechanic, Elec, Science, Math, Arith, Word, Parag, Numer)

asvab <- as.data.frame(scale(asvab, center = T, scale = T))
pc.asvab <- prcomp(asvab, scale = T)

names(asvab)

pc.asvab.loading <- pc.asvab$rotation
pc.asvab.loading

iq %<>%
  mutate(ASVAB_PC1 = pc.asvab$x[,1],
         ASVAB_PC2 = pc.asvab$x[,2],)
```

```{r}
plot(summary(pc.asvab)$importance[2, ],
     ylab="PVE",
     xlab="Number of PCs",
     pch = 16, 
     main="Scree Plot of PVE for ASVAB")
```
     

ii. Is there any evidence showing ASVAB test scores in terms of ASVAB_PC1 and ASVAB_PC2, might affect the Income?  Show your work here. You may control a few other variables, including gender. 

```{r}
fit1 <- lm(Income ~ ASVAB_PC1 + ASVAB_PC2, iq)
summary(fit1)
```

```{r}
fit.gender <- lm(Income ~ ASVAB_PC1 + ASVAB_PC2 + Gender, iq)
summary(fit1)
Anova(fit.gender)
```

```{r}
fit.edu <- lm(Income ~ ASVAB_PC1 + ASVAB_PC2 + Gender + Educ, iq)
summary(fit1)

Anova(fit.edu)
```

```{r}
fit.race <- lm(Income ~ ASVAB_PC1 + ASVAB_PC2 + Race, iq)
summary(fit.race)
Anova(fit.race)

```
```{r}
fit.fam.inc <- lm(Income ~ ASVAB_PC1 + ASVAB_PC2 + FamilyIncome78, iq)
summary(fit.fam.inc)
Anova(fit.fam.inc)

```

```{r}
fit.all <- lm(Income ~ . - Science - Arith - Word - Parag - Numer - Coding - Auto - Math - Mechanic - Elec - AFQT , iq)
summary(fit.all)
Anova(fit.all)
```

iii. Is there any evidence to show that there is gender bias against either male or female in terms of income in the above model? 

_yes_

We next build a few models for the purpose of prediction using all the information available. From now on you may use the three data sets setting (training/testing/validation) when it is appropriate. 

## 3. Trees

i. fit1: tree(Income ~ Educ + Gender, data.train) with default set up 

    a) Display the tree
    b) How many end nodes? Briefly explain how the estimation is obtained in each end nodes and deescribe the prediction equation
    c) Does it show interaction effect of Gender and Educ over Income?
    d) Predict Michelle's income
    
```{r}
fit.tree <- tree(Income ~ Educ + Gender, iq.train)
plot(fit.tree)
text(fit.tree, pretty=0)
```
```{r}
summary(fit.tree)
```
```{r}
exp(predict(fit.tree, Michelle))
```


ii. fit2: fit2 <- rpart(Income2005 ~., data.train, minsplit=20, cp=.009)

    a) Display the tree using plot(as.party(fit2), main="Final Tree with Rpart") 
    b) A brief summary of the fit2
    c) Compare testing errors between fit1 and fit2. Is the training error from fit2 always less than that from fit1? Is the testing error from fit2 always smaller than that from fit1? 
    d) You may prune the fit2 to get a tree with small testing error. 
    
```{r}
fit.tree2 <- rpart(Income ~., iq.train, minsplit=20, cp=.009)
plot(as.party(fit.tree2), main="Final Tree with Rpart")
```

```{r}
summary(fit.tree2)
```


```{r}
test.error.tree <- sum((predict(fit.tree, iq.test) - iq.test$Income)^2)
test.error.tree
test.error.tree2 <- sum((predict(fit.tree2, iq.test)-iq.test$Income)^2)
test.error.tree2
```

    
iii. fit3: bag two trees

    a) Take 2 bootstrap training samples and build two trees using the 
    rpart(Income2005 ~., data.train.b, minsplit=20, cp=.009). Display both trees.
    b) Explain how to get fitted values for Michelle by bagging the two trees obtained above. Do not use the predict(). 
    c) What is the testing error for the bagged tree. Is it guaranteed that the testing error by bagging the two tree always smaller that either single tree? 
    
```{r results = 'hold'}
# bootstrap tree 1 
par(mfrow=c(1, 2))
n=1000
set.seed(1)  
index1 <- sample(n, n, replace = TRUE)
iq.train.a <- iq.train[index1, ]

fit.tree.3a <- rpart(Income ~., iq.train.a, minsplit=20, cp=.009)
plot(fit.tree.3a, main = "Trees by rpart")
text(fit.tree.3a, pretty = TRUE) 

# bootstrap tree 2 
set.seed(2)  
index2 <- sample(n, n, replace = TRUE)
iq.train.b <- iq.train[index2, ]

fit.tree.3b <- rpart(Income ~., iq.train.b, minsplit=20, cp=.009)
plot(fit.tree.3b, main = "Trees by rpart")
text(fit.tree.3b, pretty = TRUE) 
```

    
iv. fit4: Build a best possible RandomForest 

    a) Show the process how you tune mtry and number of trees. Give a very high level explanation how fit4 is built.
    b) Compare the oob errors form fit4 to the testing errors using your testing data. Are you convinced that oob errors estimate testing error reasonably well.
    c) What is the predicted value for Michelle?

    
v. Now you have built so many predicted models (fit1 through fit4 in this section). What about build a fit5 which bags fit1 through fit4. Does fit5 have the smallest testing error?

vi.  Summarize the results and nail down one best possible final model you will recommend to predict income. Explain briefly why this is the best choice. Finally for the first time evaluate the prediction error using the validating data set. 

vii. Use your final model to predict Michelle's income. 


    





